
\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{upquote}
\usepackage[margin=20mm]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{placeins}
\usepackage{enumitem}

\newenvironment{statement}[2][Statement]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\usepackage{xcolor}

\usepackage{subfigure}


% Listings package for code rendering (No external dependencies)
\usepackage{listings}  
\usepackage{xcolor}   % Color support
\usepackage{tcolorbox} % Box for better appearance

% Define custom colors for code highlighting
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstset{frame=tb,
    language=Python,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    keepspaces=true,                 
    numbers=left,       
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
}





\title{Assignment 5}


\author{Author \\
 Wanjing Hu / fng685@alumni.ku.dk  \\
 Shuangcheng Jia / bkg713@alumni.ku.dk/   \\
 Zhigao Yan / sxd343@alumni.ku.dk  \\
} 

\begin{document}
\maketitle

\section{Inspecting Spectrograms}
%Wanjing
The spectrogram visualizes the time-frequency representation of the audio signal. The brightest (high energy) horizontal bands indicate the fundamental frequencies of the played notes. By comparing the observed frequencies with reference values from Table 1, we can determine the musical notes present in the signal.

From the spectrogram analysis, the following fundamental frequencies were identified:\\
C (261.63 Hz)\\
E (329.63 Hz)\\
G (392.00 Hz)\\
A (440.00 Hz)\\
These frequencies correspond to the notes played in the analyzed audio sample.

The choice of window size in the Short-Time Fourier Transform (STFT) significantly affects the spectrogram's resolution:\\
Larger window sizes (2048, 4096 samples) provide better frequency resolution but result in poorer time resolution.\\
Smaller window sizes (256, 512 samples) improve time resolution but reduce frequency resolution.\\
Trade-off: A window size of 1024 samples was selected as it provides a good balance between time and frequency resolution.

The following Python code was used to generate the spectrogram and extract the fundamental frequencies:
\begin{lstlisting}
sample_rate, audio = wav.read("../TestSignals/Sound samples/progression.wav")
# Convert to mono if stereo
if len(audio.shape) > 1:
    audio = np.mean(audio, axis=1)
# Define STFT parameters
window_size = 1024  # You can modify this to see the impact
hop_size = window_size // 2
window = signal.windows.hann(window_size)

# Compute Short-Time Fourier Transform (STFT)
frequencies, times, spectrogram = signal.spectrogram(audio, fs=sample_rate, 
                                                     window=window, 
                                                     nperseg=window_size, 
                                                     noverlap=hop_size, 
                                                     mode='magnitude')

# Filter to show only 0-1000 Hz
freq_limit = 1000
freq_mask = frequencies <= freq_limit

# Plot the spectrogram
plt.figure(figsize=(10, 6))
plt.pcolormesh(times, frequencies[freq_mask], spectrogram[freq_mask], shading='gouraud', cmap='inferno')
plt.colorbar(label="Magnitude")
plt.xlabel("Time (s)")
plt.ylabel("Frequency (Hz)")
plt.title("Spectrogram of progression.wav (0-1000 Hz)")
plt.show()
\end{lstlisting}

\section{Inverse filtering}
%zhigao
\subsection{}
\begin{lstlisting}
def LSI(image, kernel, noise_image):

    # convolution
    degraded = convolve2d(image, kernel, mode='same', boundary='symm')
    
    # Adding noise
    degraded += noise_image

    return degraded
\end{lstlisting}
Based on the formula:
\[g(x,y) = (f*g)(x,y)+n(x,y)\]
Firstly, a convolution is done using the filter kernel and the original image, then the noise image is added, and finally the degraded image is output.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{pics/a5_2.1_1.png} 
    \caption{LSI for image with noise}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{pics/a5_2.1_2.png} 
    \caption{LSI for image with noise2}
\end{figure}


First, I set up two kernels, a Gaussian kernel(5x5) and an Mean kernel(5x5). Secondly there are also two types of noise which are mean 0 variance 2.5 and mean 0 and variance 10.
It can be observed that when the noise is low the image is just blurred and when the noise is high the image is dominated by the noise.
\subsection{}
\begin{lstlisting}
    
def inverse_filtering(degraded_image, psf):

    # Fourier transform
    G = fft2(degraded_image)
    H = fft2(psf, s=degraded_image.shape)

    # F(u,v) = G(u,v)/H(u,v)
    F = G / H

    # Fourier inverse transformation.
    restored_image = np.real(ifft2(F))

    return restored_image  

\end{lstlisting}

Based on the formula:
\[F(u,v) = G(u,v)/H(u,v)\]
First do a Fourier transform on the degraded image as well as the psf, the filter kernel used in the previous question. Then the calculation is based on the formula, and finally Fourier inverse transform is done to convert the image to the frequency domain, and then output.
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{pics/a5_2.2.png} 
    \caption{Direct inverse filtering for different level of noise}
\end{figure}


I used the degraded image from the previous question using the Gaussian kernel and the LSI model. The noise remains the same, which are mean 0 variance 2.5 and mean 0 and variance 10.
I think direct inverse filtering basically fails to restore the original image if noise is present. We can see that when the noise is low some of the original image is still present, if the noise is high then the restored image is completely hidden by the noise.


\subsection{}
\begin{lstlisting}
    
def wiener_filter(degraded_image, psf, K):

    # Fourier transform
    G = fft2(degraded_image)
    H = fft2(psf, s=degraded_image.shape)

    F_hat = (np.abs(H)**2 / (np.abs(H)**2 + K)) * (1/H) * G
   

    # Fourier inverse transformation.
    restored_image = np.real(ifft2(F_hat))

    return restored_image   

\end{lstlisting}
Based on the formula:
\[F(u,v) = G(u,v)/\hat{H}(u,v)\]
\[
\frac{1}{\hat{H}(u,v)}
= \frac{1}{H(u,v)} \cdot \frac{\lvert H(u,v)\rvert^2}{\lvert H(u,v)\rvert^2 + K}
\]
The degraded image, the filter kernel (PSF) used, and K(Signal-to-noise-ratio) are inputted in the first. Fourier transform them separately. Then the degraded image is calculated according to the formula and output after inverse transformation.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{pics/a5_2.3.png} 
    \caption{Wiener filter with different K and noise level}
\end{figure}
I think that Wiener filtering is much better for recovery, especially when faced with a lot of noise. Firstly the noise settings are the same as in the previous question, I set K to 0.01 and 0.1.
It can be observed that the recovery is good for low noise. For high noise, the recovered image is not completely covered by the noise but shows the outline of the original image.

\section{Fixed scale feature detectors}
%Wanjing
%3.1
\subsection{}

\textbf{sigma}: Controls the standard deviation of the Gaussian filter. Larger values (e.g., 3) increase smoothing, reducing noise at the cost of edge sharpness. This results in thicker, fewer edges due to blurred gradients. See result in Figure 5.

\textbf{low\_threshold/high\_threshold}: Define gradient magnitude thresholds for edge connectivity. Lower values (e.g., 0.05/0.2) detect faint edges and noise, while higher values retain only prominent edges. The high threshold typically follows the rule of thumb: \( \text{high\_threshold} \approx 2-3 \times \text{low\_threshold} \).

\begin{lstlisting}
# Load grayscale image
image = skimage.io.imread('../TestImages/Week 1/hand.tiff', as_gray=True)

# Edge detection with default parameters (sigma=1)
edges_default = skimage.feature.canny(image)

# Increased smoothing (sigma=3)
edges_sigma3 = skimage.feature.canny(image, sigma=3)

# Lower thresholds for more edge sensitivity
edges_low_high = skimage.feature.canny(image, low_threshold=0.05, high_threshold=0.2)
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{pics/a5-3.1} 
    \caption{Canny edge detection results. From left: default parameters (sigma=1), sigma=3 (blurred edges), and low thresholds (0.05/0.2) showing increased noise.}
\end{figure}

% 3.2
\subsection{}
\textbf{sigma}: Scales the Gaussian filter for gradient computation. Larger sigma (e.g., 3) smooths gradients over broader regions, detecting corners at coarser scales. See result in Figure 6.

\textbf{k}: Sensitivity factor for the corner response \( R \). Higher values (e.g., 0.2) suppress false positives but may discard weaker corners. 

\textbf{method='eps'}: Switches to Nobleâ€™s corner measure \( R = \frac{\lambda_1 \lambda_2}{\epsilon + \lambda_1 + \lambda_2} \). Lower \(\epsilon\) (e.g., 0.01) increases sensitivity to corner-like structures.

\begin{lstlisting}
image_house = skimage.io.imread('../TestImages/Week 1/modelhouses.png', as_gray=True)

# Baseline parameters: sigma=1, k=0.05, method='k' (Harris original)
harris1 = skimage.feature.corner_harris(image_house, sigma=1, k=0.05, method='k')

# Coarser scale (sigma=3)
harris2 = skimage.feature.corner_harris(image_house, sigma=3, k=0.05, method='k')

# Higher k suppresses weak corners
harris3 = skimage.feature.corner_harris(image_house, sigma=1, k=0.2, method='k')

# Noble's method with low epsilon for sensitivity
harris4 = skimage.feature.corner_harris(image_house, sigma=1, method='eps', eps=0.01)

# Plot responses
fig, axes = plt.subplots(2, 2, figsize=(10, 10))
axes[0,0].imshow(harris1, cmap='viridis')
axes[0,0].set_title('sigma=1, k=0.05')
axes[0,1].imshow(harris2, cmap='viridis')
axes[0,1].set_title('sigma=3, k=0.05')
axes[1,0].imshow(harris3, cmap='viridis')
axes[1,0].set_title('sigma=1, k=0.2')
axes[1,1].imshow(harris4, cmap='viridis')
axes[1,1].set_title('sigma=1, eps=0.01 (Noble)')
plt.show()
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{pics/a5-3.2} 
    \caption{Harris corner response maps. Top-left: baseline. Top-right: coarse scale. Bottom-left: high k. Bottom-right: Nobleâ€™s method with low epsilon.}
\end{figure}

% 3.3
\subsection{}

The 250 strongest corners (red crosses) are localized at building edges, roof intersections, and structural vertices. Parameters (sigma=1, k=0.05, min\_distance=1) balance sensitivity and precision, with min\_distance ensuring spatial separation between peaks. See result in Figure 7.

\begin{lstlisting}
def find_harris_corners(image, sigma=1, k=0.05, method='k', min_distance=1, num_peaks=250):
    from skimage.feature import corner_harris, corner_peaks
    # Compute corner response
    response = corner_harris(image, sigma=sigma, k=k, method=method)
    # Extract top peaks with minimum pixel separation
    coords = corner_peaks(response, min_distance=min_distance, num_peaks=num_peaks)
    return coords

# Detect corners on modelhouses.png
coords = find_harris_corners(image_house, sigma=1, k=0.05)
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{pics/a5-3.3} 
    \caption{250 strongest Harris corners overlaid on modelhouses.png. Corners cluster at high-curvature points and structural intersections.}
\end{figure}

\section{Scale-space blob detector}
%Shuangcheng1
\subsection{}
The following code demonstrates the convolution property of 2D Gaussian functions by generating, convolving, and comparing Gaussian blob images:
\begin{lstlisting}[caption={Code for 4.1},captionpos=b]
# 1. Define the 2D Gaussian function
def gaussian(x, y, sigma):
    return (1 / (2 * np.pi * sigma**2)) * np.exp(-(x**2 + y**2) / (2 * sigma**2))
# 2. Generate a blob image using the Gaussian function
def model_blob_image(x, y, sigma): return gaussian(x, y, sigma)
# 3. Apply Gaussian convolution to the blob image
def convolve_blob_image(original_blob, gaussian_filter):
    return convolve2d(original_blob, gaussian_filter, mode='same')

grid_size = 30
x = np.arange(-grid_size // 2, grid_size // 2 + 1)
y = np.arange(-grid_size // 2, grid_size // 2 + 1)
x_coords, y_coords = np.meshgrid(x, y)

# Generate the initial blob image with sigma = 1
sigma = 1
blob_image_original = model_blob_image(x_coords, y_coords, sigma)

# Generate the Gaussian filter for convolution with tau = 2
tau = 2
gaussian_filter = model_blob_image(x_coords, y_coords, tau)

blob_image_convolved = convolve_blob_image(blob_image_original, gaussian_filter)

gamma = np.sqrt(sigma**2 + tau**2)
blob_image_calculated = model_blob_image(x_coords, y_coords, gamma)

difference_image = blob_image_calculated - blob_image_convolved

## The visualization part is not described here.

\end{lstlisting}

\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\columnwidth, keepaspectratio]{pics/a5-4.1.png}
    \caption[]{Result for 4.1}
    \label{fig:4.1}
\end{figure}

\FloatBarrier
To verify the convolution property of two-dimensional Gaussian functions:
    $G(x,y,\sigma) * G(x,y,\tau) = G(x,y, \sqrt{\sigma^2 + \tau^2}),
$
we construct and visualize different Gaussian blob images to confirm this property.

Figure \ref{fig:4.1} presents the results. The first subplot shows the original blob image with scale $\sigma = 1$. The second subplot from the left is the convolved blob image, obtained by applying a Gaussian filter with scale $\tau = 2$ to the original blob. The third subplot is the calculated blob image, generated directly using a Gaussian function with scale $\gamma = \sqrt{\sigma^2 + \tau^2} \approx 2.24$. The last subplot shows the difference image, computed as the calculated blob image minus the convolved blob image.

From the difference image, we observe that the center region is dark, indicating that the convolved blob image has slightly lower pixel values than the calculated blob image due to Gaussian smoothing. The edges of the central region are relatively bright, meaning the convolved image has higher pixel values in those areas, likely due to intensity spread during convolution. These differences arise from numerical approximations and edge effects but do not contradict the theoretical formulation.

The results confirm that convolving a Gaussian function with another Gaussian produces a new Gaussian with an increased standard deviation as predicted by the convolution theorem.

\subsection{}
The two-dimensional Gaussian function is:
\begin{equation}
    G(x,y,\sigma) = \frac{1}{2\pi\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2} \right).
\end{equation}
Taking its Fourier transform:
\begin{equation}
    \mathcal{F} \{ G(x,y,\sigma) \} = \exp\left(-\frac{1}{2} \sigma^2 (k_x^2 + k_y^2) \right).
\end{equation}
This shows that the Fourier transform of a Gaussian remains a Gaussian in frequency space.
Next, the convolution theorem states that the Fourier transform of a convolution is the product of the individual Fourier transforms:
\begin{equation}
    \mathcal{F} \{ G(x,y,\sigma) * G(x,y,\tau) \} = \mathcal{F} \{ G(x,y,\sigma) \} \cdot \mathcal{F} \{ G(x,y,\tau) \}.
\end{equation}
Substituting the Gaussian Fourier transforms:
\begin{equation}
    \exp\left(-\frac{1}{2} \sigma^2 (k_x^2 + k_y^2) \right) \cdot \exp\left(-\frac{1}{2} \tau^2 (k_x^2 + k_y^2) \right).
\end{equation}
Using exponent addition:
\begin{equation}
    \exp\left(-\frac{1}{2} (\sigma^2 + \tau^2) (k_x^2 + k_y^2) \right).
\end{equation}
This is the Fourier transform of a Gaussian with standard deviation:
\begin{equation}
    \sqrt{\sigma^2 + \tau^2}.
\end{equation}
Since the inverse Fourier transform of this expression is again a Gaussian with the updated standard deviation, we conclude:
\begin{equation}
    G(x,y,\sigma) * G(x,y,\tau) = G(x,y,\sqrt{\sigma^2 + \tau^2}).
\end{equation}

\subsection{}

\begin{enumerate}[label=\roman*., leftmargin=1cm]
    \item
We aim to derive the closed-form expression for the scale-normalized Laplacian:

\begin{equation}
    H(x,y,\tau) = I_{xx}(x,y,\tau) + I_{yy}(x,y,\tau).
\end{equation}

From the definition of scale-normalized derivatives:
\begin{equation}
    I_{x^m y^n}(x, y, \tau) = \tau^{\gamma(m+n)} \frac{\partial^{m+n} I(x,y,\tau)}{\partial x^m \partial y^n},
\end{equation}
set \( \gamma = 1 \), we obtain:
\begin{equation}
    I_{xx}(x,y,\tau) = \tau^2 \frac{\partial^2 I(x,y,\tau)}{\partial x^2}, \quad
    I_{yy}(x,y,\tau) = \tau^2 \frac{\partial^2 I(x,y,\tau)}{\partial y^2}.
\end{equation}
Thus, the scale-normalized Laplacian is:
\begin{equation}
    H(x,y,\tau) = \tau^2 \left( \frac{\partial^2 I(x,y,\tau)}{\partial x^2} + \frac{\partial^2 I(x,y,\tau)}{\partial y^2} \right).
\end{equation}
Use the convolution property of Gaussians:
\begin{equation}
    I(x,y,\tau) = G(x,y,\sigma) * G(x,y,\tau) = G(x,y,\sqrt{\sigma^2 + \tau^2}),
\end{equation}
Express it as:
\begin{equation}
    I(x,y,\tau) = \frac{1}{2\pi(\sigma^2 + \tau^2)} e^{-\frac{x^2 + y^2}{2(\sigma^2 + \tau^2)}}.
\end{equation}
Use the known formula for the second derivative of a Gaussian:
\begin{equation}
    \frac{\partial^2 I}{\partial x^2} = - \left(\frac{1}{2\pi (\sigma^2 + \tau^2)^2}\right) \left(1 - \frac{x^2}{\sigma^2 + \tau^2} \right) e^{-\frac{x^2 + y^2}{2(\sigma^2 + \tau^2)}},
\end{equation}
and similarly for \( y \):
\begin{equation}
    \frac{\partial^2 I}{\partial y^2} = - \left(\frac{1}{2\pi (\sigma^2 + \tau^2)^2}\right) \left(1 - \frac{y^2}{\sigma^2 + \tau^2} \right) e^{-\frac{x^2 + y^2}{2(\sigma^2 + \tau^2)}}.
\end{equation}
Sum both derivatives:
\begin{equation}
    \frac{\partial^2 I}{\partial x^2} + \frac{\partial^2 I}{\partial y^2} = - \left(\frac{1}{2\pi (\sigma^2 + \tau^2)^2}\right) \left(2 - \frac{x^2 + y^2}{\sigma^2 + \tau^2} \right) e^{-\frac{x^2 + y^2}{2(\sigma^2 + \tau^2)}}.
\end{equation}
Multiplying by \( \tau^2 \), we obtain:
\begin{equation}
    H(x,y,\tau) = - \left( \frac{\tau^2}{2\pi (\sigma^2 + \tau^2)^2} \right) \left(2 - \frac{x^2 + y^2}{\sigma^2 + \tau^2} \right) e^{-\frac{x^2 + y^2}{2(\sigma^2 + \tau^2)}}.
\end{equation}
    \item
    At the point \( (0,0) \), the scale-normalized Laplacian is given by
\begin{equation}
    H(0,0,\tau) = -\frac{\tau^2}{\pi (\sigma^2 + \tau^2)^2}.
\end{equation}

To find the extremal points, we differentiate with respect to \( \tau \):
\begin{equation}
    \frac{dH}{d\tau} = \frac{4\tau^3}{\pi (\sigma^2 + \tau^2)^3} - \frac{2\tau}{\pi (\sigma^2 + \tau^2)^2}.
\end{equation}

Setting \( \frac{dH}{d\tau} = 0 \), we factor out \( \tau \), obtaining the solutions
\begin{equation}
    \tau = 0, \quad \tau = \pm\sigma.
\end{equation}

To classify these points, we compute the second derivative:
\begin{equation}
    \frac{d^2H}{d\tau^2} = -\frac{24\tau^4}{\pi (\sigma^2 + \tau^2)^4} + \frac{20\tau^2}{\pi (\sigma^2 + \tau^2)^3} - \frac{2}{\pi (\sigma^2 + \tau^2)^2}.
\end{equation}

Evaluating at \( \tau = 0 \), we get
\begin{equation}
    \frac{d^2H}{d\tau^2} \Big|_{\tau=0} = -\frac{2}{\pi \sigma^4} < 0,
\end{equation}
indicating a maximum. For \( \tau = \pm\sigma \),
\begin{equation}
    \frac{d^2H}{d\tau^2} \Big|_{\tau = \pm \sigma} = \frac{1}{2\pi \sigma^4} > 0,
\end{equation}
indicating minima.

Thus, \( H(0,0,\tau) \) has a maximum at \( \tau = 0 \) and minima at \( \tau = \pm\sigma \). There are no saddle points.

    \item
Figure \ref{fig:4.3} shows \( H(0,0,\tau) \) as a function of \( \tau \), with vertical dashed lines marking \( \tau = 1,2,3 \) and their negatives.

    \FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\columnwidth, keepaspectratio]{pics/a5-4.3.png}
    \caption[]{Figure for 4.3}
    \label{fig:4.3}
\end{figure}

\FloatBarrier

The function reaches a maximum at \( \tau = 0 \), confirming our analysis. The minima occur at \( \tau = \pm\sigma \) (here, \( \sigma = 1 \)), consistent with theoretical predictions. The dashed lines highlight different scales where \( H(0,0,\tau) \) is evaluated. As \( \tau \) increases, \( H(0,0,\tau) \) approaches zero, indicating reduced response at large scales.

This confirms that \( H(0,0,\tau) \) has a peak at \( \tau = 0 \) and dips at characteristic scales, with no saddle points.
\end{enumerate}

\subsection{}
The code shows the blob detection based on the scale-normalized Laplacian of Gaussian (LoG).
\begin{lstlisting}[caption={Code for 4.4},captionpos=b]
## Defining parameters part is omitted here

# Compute the scale-normalized Laplacian and detect extrema
for sigma in sigma_values:
    H = sigma**(2 * gamma)*(gaussian_filter(image, sigma=sigma, order=(2, 0)) +
                            gaussian_filter(image, sigma=sigma, order=(0, 2)))

    # Detect local maxima and minima
    maxima = feature.peak_local_max(H, min_distance=10, threshold_abs=0.02, exclude_border=False)
    minima = feature.peak_local_max(-H, min_distance=5, threshold_abs=0.01, exclude_border=False)

    # Store detected points with their scale
    for y, x in maxima:
        blobs.append((y, x, sigma, H[y, x]))
    for y, x in minima:
        blobs.append((y, x, sigma, H[y, x]))

# Sort blobs by absolute response value and select the strongest
blobs.sort(key=lambda b: -abs(b[3]))
blobs_maxima = blobs[:num_blobs]
blobs_minima = blobs[-num_blobs:]

## The visualization part is not described here.

\end{lstlisting}

Fig \ref{fig:4.4} illustrates the detection results on the sunflower.tiff image.


\FloatBarrier

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\columnwidth, keepaspectratio]{pics/a5-4.4.png}
    \caption[]{Result for 4.4}
    \label{fig:4.4}
\end{figure}

\FloatBarrier

Maxima of \( H(x,y,\tau) \) represent bright blob-like structures in the image, corresponding to regions that are brighter than their surroundings, such as the bright petals of the sunflowers or the sky. Minima of \( H(x,y,\tau) \) represent dark blob-like structures, corresponding to regions that are darker than their surroundings, such as the dark centers of the sunflowers.

Thus, maxima highlight bright object regions, while minima emphasize dark object regions.

\end{document}
